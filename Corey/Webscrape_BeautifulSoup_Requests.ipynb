{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf1c4831",
   "metadata": {},
   "source": [
    "### BeautifulSoup - \n",
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.\n",
    "\n",
    "#### !pip install beautifulsoup4\n",
    "\n",
    "\n",
    "They also suggest using the lxml parser\n",
    "\n",
    "parse - resolve (a sentence) into its component parts and describe their syntactic roles.\n",
    "#### !pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4722e886",
   "metadata": {},
   "source": [
    "### Requests -\n",
    "Requests allows you to send HTTP/1.1 requests extremely easily. Thereâ€™s no need to manually add query strings to your URLs, or to form-encode your POST data. Keep-alive and HTTP connection pooling are 100% automatic, thanks to urllib3.\n",
    "#### !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "facbb475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"no-js\" lang=\"\">\n",
      " <head>\n",
      "  <title>\n",
      "   Test - A Sample Website\n",
      "  </title>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <link href=\"css/normalize.css\" rel=\"stylesheet\"/>\n",
      "  <link href=\"css/main.css\" rel=\"stylesheet\"/>\n",
      " </head>\n",
      " <body>\n",
      "  <h1 id=\"site_title\">\n",
      "   Test Website\n",
      "  </h1>\n",
      "  <hr/>\n",
      "  <div class=\"article\">\n",
      "   <h2>\n",
      "    <a href=\"article_1.html\">\n",
      "     Article 1 Headline\n",
      "    </a>\n",
      "   </h2>\n",
      "   <p>\n",
      "    This is a summary of article 1\n",
      "   </p>\n",
      "  </div>\n",
      "  <hr/>\n",
      "  <div class=\"article\">\n",
      "   <h2>\n",
      "    <a href=\"article_2.html\">\n",
      "     Article 2 Headline\n",
      "    </a>\n",
      "   </h2>\n",
      "   <p>\n",
      "    This is a summary of article 2\n",
      "   </p>\n",
      "  </div>\n",
      "  <hr/>\n",
      "  <div class=\"footer\">\n",
      "   <p>\n",
      "    Footer Information\n",
      "   </p>\n",
      "  </div>\n",
      "  <script src=\"js/vendor/modernizr-3.5.0.min.js\">\n",
      "  </script>\n",
      "  <script src=\"js/plugins.js\">\n",
      "  </script>\n",
      "  <script src=\"js/main.js\">\n",
      "  </script>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "with open ('simple.html') as html_file:\n",
    "    soup = BeautifulSoup(html_file, 'lxml')\n",
    "    \n",
    "    \n",
    "# print(soup)\n",
    "\n",
    "# with indentation\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ebaa90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - A Sample Website\n"
     ]
    }
   ],
   "source": [
    "# Get the title, access it like an attribute...\n",
    "# match = soup.title.text\n",
    "\n",
    "match = soup.title\n",
    "\n",
    "# print(match)\n",
    "\n",
    "# only text\n",
    "print(match.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "067c0109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"footer\">\n",
      "<p>Footer Information</p>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "# Search for specific divs using classes or IDs...\n",
    "\n",
    "# match = soup.find_all('div')             -- for all divs\n",
    "\n",
    "# match = soup.find('div', id = 'footer')\n",
    "\n",
    "match = soup.find('div', class_ = 'footer')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88aa0a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"article\">\n",
      "<h2><a href=\"article_1.html\">Article 1 Headline</a></h2>\n",
      "<p>This is a summary of article 1</p>\n",
      "</div> \n",
      "\n",
      "\n",
      "Article 1 Headline \n",
      "\n",
      "\n",
      "article_1.html\n"
     ]
    }
   ],
   "source": [
    "# getting the link inside the article...\n",
    "article = soup.find('div', class_ = 'article')\n",
    "print(article, '\\n\\n')\n",
    "\n",
    "# Get text\n",
    "link_text = article.h2.a.text\n",
    "print(link_text, '\\n\\n')\n",
    "\n",
    "# get link\n",
    "link = article.h2.a['href']\n",
    "print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd8ef66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"article\">\n",
      "<h2><a href=\"article_1.html\">Article 1 Headline</a></h2>\n",
      "<p>This is a summary of article 1</p>\n",
      "</div> \n",
      "\n",
      "\n",
      "<div class=\"article\">\n",
      "<h2><a href=\"article_2.html\">Article 2 Headline</a></h2>\n",
      "<p>This is a summary of article 2</p>\n",
      "</div> \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looping through all articles...\n",
    "\n",
    "for article in soup.find_all('div', class_ = 'article'):\n",
    "    print(article, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f8e78",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e4e2c",
   "metadata": {},
   "source": [
    "# Working with websites..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2282ecbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/embed/z0gguhEmWiY\n"
     ]
    }
   ],
   "source": [
    "source = requests.get('http://coreyms.com').text\n",
    "\n",
    "soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "# print(soup.prettify())  -- entire html page\n",
    "\n",
    "# finding article tag  --returns the first one...\n",
    "article = soup.find('article')\n",
    "\n",
    "# print(article.prettify(), '\\n\\n\\n')\n",
    "\n",
    "\n",
    "# get headline - \n",
    "# headline = article.h2.a.text\n",
    "# print(headline)\n",
    "\n",
    "\n",
    "# get paragraph from class \"entry-content\" - \n",
    "# summary = article.find('div', class_ = 'entry-content').p.text\n",
    "# print(summary)\n",
    "\n",
    "\n",
    "# get youtube vid link - \n",
    "vid_src = article.find('iframe', class_ = 'youtube-player')['src']\n",
    "# print(vid_src)\n",
    "\n",
    "\n",
    "# Splitting string based on /\n",
    "vid_id = vid_src.split('/')[4]\n",
    "# print(vid_id)\n",
    "\n",
    "\n",
    "vid_id = vid_src.split('?')[0]\n",
    "print(vid_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fedff4f",
   "metadata": {},
   "source": [
    "## For all articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49d7242a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/embed/z0gguhEmWiY\n",
      "https://www.youtube.com/embed/_P7X8tMplsw\n",
      "https://www.youtube.com/embed/fKl2JW_qrso\n",
      "https://www.youtube.com/embed/IEEhzQoKtQU\n",
      "https://www.youtube.com/embed/mO_dS3rXDIs\n",
      "https://www.youtube.com/embed/2Fp1N6dof0Y\n",
      "https://www.youtube.com/embed/-nh9rCzPJ20\n",
      "https://www.youtube.com/embed/06I63_p-2A4\n",
      "https://www.youtube.com/embed/_JGmemuINww\n"
     ]
    }
   ],
   "source": [
    "source = requests.get('http://coreyms.com').text\n",
    "\n",
    "soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "# print(soup.prettify())  -- entire html page\n",
    "\n",
    "# finding article tag  \n",
    "for article in soup.find_all('article'):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # print(article.prettify(), '\\n\\n\\n')\n",
    "\n",
    "\n",
    "        # get headline - \n",
    "        # headline = article.h2.a.text\n",
    "        # print(headline)\n",
    "\n",
    "\n",
    "        # get paragraph from class \"entry-content\" - \n",
    "        # summary = article.find('div', class_ = 'entry-content').p.text\n",
    "        # print(summary)\n",
    "\n",
    "\n",
    "        # get youtube vid link - \n",
    "        vid_src = article.find('iframe', class_ = 'youtube-player')['src']\n",
    "        # print(vid_src)\n",
    "\n",
    "\n",
    "        # Splitting string based on /\n",
    "        vid_id = vid_src.split('/')[4]\n",
    "        # print(vid_id)\n",
    "\n",
    "\n",
    "        vid_id = vid_src.split('?')[0]\n",
    "        print(vid_id)\n",
    "        \n",
    "    except Exception as e:\n",
    "        vid_id = None\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911cb5e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
